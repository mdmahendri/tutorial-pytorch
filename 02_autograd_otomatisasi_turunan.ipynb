{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOGRAD: Auto Gradient\n",
    "Pytorch menyebut package `autograd` pada pytorch merupakan bagian paling penting pada neural network. Mengingat tiap arsitektur neural networks pasti membutuhkan turunan/gradien. `autograd` bertugas menghitung turunan semua operasi yang dilakukan pada tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "- `torch.Tensor` akan merekam riwayat gradien jika kita set atribut `.requires_grad` bernilai `True` dan bisa diakses lewat atribut `.grad`\n",
    "- Riwayat gradien tensor berasal dari perhitungan otomatis semua gradien oleh fungsi `.backward()`\n",
    "- Jika tidak menginginkan perekaman riwayat lagi maka bisa memanggil `.detach()`\n",
    "- Atau melalui `with torch.no_grad():`, khususnya ini berguna saat finetune pretrained model karena ada yang perlu difreeze\n",
    "- Selain `Tensor` komponen lain penyusun graf komputasi adalah `Function`, mirip-mirip tensorflow lah\n",
    "- Untuk mengetahui `Function` apa yang membentuk suatu `Tensor` dapat dilihat pada atribut `.grad_fn`\n",
    "\n",
    "### Rencana Implementasi\n",
    "1. Membuat matriks 2x2 **X** dan rekam riwayat gradien\n",
    "2. Operasikan **Y**=**X** + 2 dilanjutkan **Z** = 3 \\* **Y** \\* **Y**\n",
    "3. Hitung mean dari semua elemen, anggap seperti agregasi loss \\[cost function\\] lalu hitung gradiennya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "<AddBackward object at 0x7f57d0e75048> <ThMulBackward object at 0x7f57d0e58ef0> <MeanBackward1 object at 0x7f57d0e589e8>\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Step 1\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "#### Step 2\n",
    "y = x + 2\n",
    "z = 3 * y * y\n",
    "\n",
    "#### Step 3\n",
    "j = z.mean()\n",
    "gradients = torch.tensor(1.) #harus dalam bentuk float\n",
    "j.backward(gradients, retain_graph=True) #atau J.backward(), retain_graph penting supaya graf komputasi tidak dihapus\n",
    "                                         #PyTorch membuat graf saat runtime bukan statis seperti tensorflow\n",
    "\n",
    "print(x)\n",
    "print(y.grad_fn, z.grad_fn, j.grad_fn)\n",
    "print(x.grad)\n",
    "x.grad.zero_() #hapus gradien pada x supaya tidak terakumulasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$j = \\frac{1}{4} \\sum_{i=1}^{4} z_i$ dan $z_i = 3(x+2)^2$ sehingga jika kita cari $\\frac{dj}{dx_1}, \\frac{dj}{dx_2}, \\frac{dj}{dx_3}, \\frac{dj}{dx_4}$ akan mendapatkan nilai 4.5  \n",
    "\n",
    "`.zero_()` diperlukan karena secara default gradien akan terakumulasi. Maksud dari terakumulasi adalah jika kita memanggil `.backward()` sebanyak 5 kali maka yang ada pada `.grad` adalah lima kali dari hasil satu kali `.backward()`. Contohnya bisa dilihat pada [forum PyTorch ini](https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903/12)  \n",
    "\n",
    "Variabel `gradients` fungsinya mirip sebagai penimbang loss, tetapi karena `j` hanya terdiri satu elemen maka tidak perlu ditulis secara eksplisit. Kita lakukan dengan contoh lain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<ThMulBackward>) tensor([[0.2500, 0.5000],\n",
      "        [0.7500, 1.0000]])\n",
      "tensor([[ 4.5000,  9.0000],\n",
      "        [13.5000, 18.0000]])\n"
     ]
    }
   ],
   "source": [
    "gradients2 = torch.tensor([[0.25, 0.5], [0.75, 1]])\n",
    "print(z, gradients2)\n",
    "\n",
    "z.backward(gradients2)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gradients2` dapat dibayangkan sebagai turunan `j` terhadap `z` dengan notasi matematikanya $\\frac{dj}{dz_1}, \\frac{dj}{dz_2}, \\frac{dj}{dz_3}, \\frac{dj}{dz_4}$. Atau seperti yang saya jelaskan di atas dapat dilihat sebagai penimbang/importance dari tiap loss. Maka tiap loss didefinisikan sebagai $.25 * \\frac{dz_1}{dx}, .5 * \\frac{dz_2}{dx}, .75 * \\frac{dz_3}{dx}, 1 * \\frac{dz_4}{dx}$ Semoga jelas ya, ini berdasar pengetahuan saya setelah membaca [stackoverflow](https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments). Jika ada yang mau koreksi silahkan\n",
    "\n",
    "Contoh lain penggunaan `gradients` ada di bawah ini. `.norm()` ini maksudnya adalah L2 norm, jika kurang dari 1000 nilai L2 norm-nya maka lakukan komputasi lagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-796.9212,  -53.3214,  621.4634], grad_fn=<MulBackward>)\n",
      "tensor([ 102.4000, 1024.0000,    0.1024])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)\n",
    "\n",
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti yang sudah disebutkan pada notebook sebelumnya tentang operasi inplace, `requires_grad` ini juga punya methodnya dengan argument defaultnya `True`. Lihat contoh di bawah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a.requires_grad_(False)\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_()\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terakhir, kita coba implementasi dari `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print((a ** 2).requires_grad) #semua output dari input yang atribut `requires_grad` bernilai True juga akan memiliki\n",
    "                              #atribut `requires_grad` bernilai True\n",
    "    \n",
    "with torch.no_grad():\n",
    "    print((a ** 2).requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
